{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30763,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer,  GenerationConfig\nimport torch\n!pip install datasets\nimport time\nimport re\nfrom difflib import SequenceMatcher","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-22T08:10:08.672575Z","iopub.execute_input":"2024-09-22T08:10:08.673044Z","iopub.status.idle":"2024-09-22T08:10:26.423843Z","shell.execute_reply.started":"2024-09-22T08:10:08.672996Z","shell.execute_reply":"2024-09-22T08:10:26.422626Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.21.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.24.6)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T08:10:26.425854Z","iopub.execute_input":"2024-09-22T08:10:26.426504Z","iopub.status.idle":"2024-09-22T08:10:27.614181Z","shell.execute_reply.started":"2024-09-22T08:10:26.426459Z","shell.execute_reply":"2024-09-22T08:10:27.613159Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"model_id1 = \"google/gemma-2b-it\"\nmodel_id2 = \"microsoft/Phi-3.5-mini-instruct\"\nmodel_id3 = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"","metadata":{"execution":{"iopub.status.busy":"2024-09-22T08:10:27.615323Z","iopub.execute_input":"2024-09-22T08:10:27.615826Z","iopub.status.idle":"2024-09-22T08:10:27.620108Z","shell.execute_reply.started":"2024-09-22T08:10:27.615790Z","shell.execute_reply":"2024-09-22T08:10:27.619225Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!pip install huggingface_hub","metadata":{"execution":{"iopub.status.busy":"2024-09-22T08:10:27.623003Z","iopub.execute_input":"2024-09-22T08:10:27.623407Z","iopub.status.idle":"2024-09-22T08:10:40.555823Z","shell.execute_reply.started":"2024-09-22T08:10:27.623360Z","shell.execute_reply":"2024-09-22T08:10:40.554819Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.24.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.7.4)\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(token=\"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\")","metadata":{"execution":{"iopub.status.busy":"2024-09-22T08:10:40.557184Z","iopub.execute_input":"2024-09-22T08:10:40.557505Z","iopub.status.idle":"2024-09-22T08:10:40.644583Z","shell.execute_reply.started":"2024-09-22T08:10:40.557462Z","shell.execute_reply":"2024-09-22T08:10:40.643657Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"# tokenizer1 = AutoTokenizer.from_pretrained(model_id1)\n# model1 = AutoModelForCausalLM.from_pretrained(model_id1).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T08:10:40.645838Z","iopub.execute_input":"2024-09-22T08:10:40.646233Z","iopub.status.idle":"2024-09-22T08:10:40.650784Z","shell.execute_reply.started":"2024-09-22T08:10:40.646187Z","shell.execute_reply":"2024-09-22T08:10:40.649820Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"tokenizer2 = AutoTokenizer.from_pretrained(model_id2)\nmodel2 = AutoModelForCausalLM.from_pretrained(model_id2).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T08:10:40.651962Z","iopub.execute_input":"2024-09-22T08:10:40.652268Z","iopub.status.idle":"2024-09-22T08:11:49.061547Z","shell.execute_reply.started":"2024-09-22T08:10:40.652233Z","shell.execute_reply":"2024-09-22T08:11:49.060728Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.98k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fba51126a2a4060b8defb1de4b38ce4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33536485af65471c8e9b1c1f0279ebb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"669be31036ae4d76ac53c59e8d3e0764"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82f6d1044e374bd886c446b7e090957f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3ddba7b27c3491186496f2e404a650f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/3.45k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"760385a320e64d8aa284c077fa0ceb8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/16.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c595074dadd444b83d0798446b5a693"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be0843989212466a81978e2d19a0e312"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91030040df354109beb8836041bea7f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76f0bfebe98a4a95a0bb7af2cfac5b6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bab818c572b474592e41128810357a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/195 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0242e18811484fc2bff3c41652906065"}},"metadata":{}}]},{"cell_type":"code","source":"# tokenizer3 = AutoTokenizer.from_pretrained(model_id3)\n# model3 = AutoModelForCausalLM.from_pretrained(model_id3).to(device) ","metadata":{"execution":{"iopub.status.busy":"2024-09-22T08:11:49.062732Z","iopub.execute_input":"2024-09-22T08:11:49.063223Z","iopub.status.idle":"2024-09-22T08:11:49.067062Z","shell.execute_reply.started":"2024-09-22T08:11:49.063188Z","shell.execute_reply":"2024-09-22T08:11:49.066210Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def generate_response(prompt,model,tokenizer,new_tokens):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n    with torch.no_grad():\n        outputs = model.generate(inputs.input_ids, max_new_tokens=new_tokens, num_return_sequences=1)\n\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return response","metadata":{"execution":{"iopub.status.busy":"2024-09-22T08:11:49.068115Z","iopub.execute_input":"2024-09-22T08:11:49.068393Z","iopub.status.idle":"2024-09-22T08:11:49.417160Z","shell.execute_reply.started":"2024-09-22T08:11:49.068363Z","shell.execute_reply":"2024-09-22T08:11:49.416153Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"dataset = load_dataset(\"cais/mmlu\",\"college_mathematics\")\ndataset = dataset['test']\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-09-22T08:11:49.420562Z","iopub.execute_input":"2024-09-22T08:11:49.420873Z","iopub.status.idle":"2024-09-22T08:11:56.801491Z","shell.execute_reply.started":"2024-09-22T08:11:49.420841Z","shell.execute_reply":"2024-09-22T08:11:56.800619Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/53.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32efd249a62442f0bb91bd761a79c1b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/138k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3085ed44ab404a7785c94925c6e4d9d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/16.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cf0974097ae43db9439e32ba1e0a715"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/5.00k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92e10962a87d4a2ba757c8a755d58b77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/5.16k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e49990a2e1a04d9180f27f0c6da028e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cf9841736514425bf4213aafd276015"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/11 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07d7a4c579c3465ca376fb7c667e6523"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating dev split:   0%|          | 0/5 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a8491426e2643dabef9b0c1bc77108e"}},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['question', 'subject', 'choices', 'answer'],\n    num_rows: 100\n})"},"metadata":{}}]},{"cell_type":"code","source":"prompts_zero_shot=[]\nfor example in dataset:\n    question = example['question']\n    options = example['choices']  \n    prompt = f\"Choose the answer to the given question from below options.\\nQuestion:{question}\\nOption 1:{options[0]}\\nOption 2:{options[1]}\\nOption 3: {options[2]}\\nOption 4: {options[3]}.\"\n    prompts_zero_shot.append(prompt)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T08:11:56.802643Z","iopub.execute_input":"2024-09-22T08:11:56.803108Z","iopub.status.idle":"2024-09-22T08:11:56.818752Z","shell.execute_reply.started":"2024-09-22T08:11:56.803074Z","shell.execute_reply":"2024-09-22T08:11:56.817904Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"prompts_cot=[]\nfor example in dataset:\n    question = example['question']\n    options = example['choices']  \n    prompt = f\"Choose the answer to the given question from below options.\\nQuestion:{question}\\nOption 1:{options[0]}\\nOption 2:{options[1]}\\nOption 3: {options[2]}\\nOption 4: {options[3]}\\nThink step by step.\"\n    prompts_cot.append(prompt)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T08:11:56.819915Z","iopub.execute_input":"2024-09-22T08:11:56.820205Z","iopub.status.idle":"2024-09-22T08:11:56.834961Z","shell.execute_reply.started":"2024-09-22T08:11:56.820174Z","shell.execute_reply":"2024-09-22T08:11:56.834115Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def extract_answer(response):\n    \n    match = re.search(r'(Therefore, the correct answer is|Therefore, the answer is|The correct answer is|the correct answer is|The answer is:|The answer is|Answer:|the answer is|the answer is:)\\s*(.*)', response)\n#     match = re.search(r'(Therefore,\\s*the\\s*correct\\s*answer\\s*is|The\\s*correct\\s*answer\\s*is|The\\s*answer\\s*is:|The\\s*answer\\s*is|Answer:|answer:)\\s*([^\\n]*)', response)\n\n    if match:\n        return match.group(2).strip()\n    return None\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T08:11:56.836059Z","iopub.execute_input":"2024-09-22T08:11:56.836363Z","iopub.status.idle":"2024-09-22T08:11:56.843278Z","shell.execute_reply.started":"2024-09-22T08:11:56.836331Z","shell.execute_reply":"2024-09-22T08:11:56.842495Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"options_val = [\"Option 1\",\"Option 2\",\"Option 3\",\"Option 4\",\"Option 1.\",\"Option 2.\",\"Option 3.\",\"Option 4.\"]","metadata":{"execution":{"iopub.status.busy":"2024-09-22T08:11:56.844347Z","iopub.execute_input":"2024-09-22T08:11:56.844725Z","iopub.status.idle":"2024-09-22T08:11:56.852181Z","shell.execute_reply.started":"2024-09-22T08:11:56.844693Z","shell.execute_reply":"2024-09-22T08:11:56.851366Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def evaluate_zeroshot(model,tokenizer,prompts,model_name,dataset):\n    responses = []\n    start = time.time()\n    for i in prompts:\n        responses.append(generate_response(i,model,tokenizer,50))\n\n    end = time.time()\n    inference_time = (end - start)/100\n    print(f\"Inference time for zero shot for {model_name}: {inference_time:.6f} seconds\")\n    \n    \n    correct_count = 0\n    for i, example in enumerate(dataset):\n        extracted_answer = extract_answer(responses[i])\n        \n        if extracted_answer==None:\n            extracted_answer = \"??\"\n        \n        if extracted_answer in options_val:\n            extracted_answer = example['choices'][int(extracted_answer[7])-1]\n\n        actual_answer = example['choices'][example['answer']]\n\n        if extracted_answer in actual_answer or actual_answer in extracted_answer:\n            correct_count += 1\n\n    accuracy = correct_count / len(dataset)\n    print(f\"Accuracy of {model_name} for zero shot: {accuracy:.2f}\")    ","metadata":{"execution":{"iopub.status.busy":"2024-09-22T08:11:56.853212Z","iopub.execute_input":"2024-09-22T08:11:56.853543Z","iopub.status.idle":"2024-09-22T08:11:56.863085Z","shell.execute_reply.started":"2024-09-22T08:11:56.853503Z","shell.execute_reply":"2024-09-22T08:11:56.862122Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def evaluate_cot(model,tokenizer,prompts,model_name,dataset):\n    responses = []\n    start = time.time()\n    for i in prompts:\n        responses.append(generate_response(i,model,tokenizer,150))\n\n    end = time.time()\n    inference_time = (end - start)/100\n    print(f\"Inference time for chain of thought for {model_name}: {inference_time:.6f} seconds\")\n    \n    correct_count = 0\n    for i, example in enumerate(dataset):\n        extracted_answer = extract_answer(responses[i])\n        if extracted_answer==None:\n            extracted_answer = \"??\"\n        \n        if extracted_answer in options_val:\n            extracted_answer = example['choices'][int(extracted_answer[7])-1]\n\n        actual_answer = example['choices'][example['answer']]\n\n        if extracted_answer in actual_answer or actual_answer in extracted_answer:\n            correct_count += 1\n\n    accuracy = correct_count / len(dataset)\n    print(f\"Accuracy of {model_name}  for cot: {accuracy:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-22T08:11:56.864291Z","iopub.execute_input":"2024-09-22T08:11:56.864802Z","iopub.status.idle":"2024-09-22T08:11:56.872373Z","shell.execute_reply.started":"2024-09-22T08:11:56.864761Z","shell.execute_reply":"2024-09-22T08:11:56.871526Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# evaluate_zeroshot(model1,tokenizer1,prompts_zero_shot,\"Gemma\",dataset)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T08:11:56.873406Z","iopub.execute_input":"2024-09-22T08:11:56.873698Z","iopub.status.idle":"2024-09-22T08:11:56.882376Z","shell.execute_reply.started":"2024-09-22T08:11:56.873668Z","shell.execute_reply":"2024-09-22T08:11:56.881597Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"evaluate_zeroshot(model2,tokenizer2,prompts_zero_shot,\"Phi-3.5\",dataset)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T08:11:56.883453Z","iopub.execute_input":"2024-09-22T08:11:56.883810Z","iopub.status.idle":"2024-09-22T08:19:55.627984Z","shell.execute_reply.started":"2024-09-22T08:11:56.883777Z","shell.execute_reply":"2024-09-22T08:19:55.626865Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nYou are not running the flash-attention implementation, expect numerical differences.\n","output_type":"stream"},{"name":"stdout","text":"Inference time for zero shot for Phi-3.5: 4.787268 seconds\nAccuracy of Phi-3.5 for zero shot: 0.27\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate_zeroshot(model3,tokenizer3,prompts_zero_shot,\"Llama-3.1\",dataset) ","metadata":{"execution":{"iopub.status.busy":"2024-09-22T08:19:55.629523Z","iopub.execute_input":"2024-09-22T08:19:55.629943Z","iopub.status.idle":"2024-09-22T08:19:55.634393Z","shell.execute_reply.started":"2024-09-22T08:19:55.629899Z","shell.execute_reply":"2024-09-22T08:19:55.633516Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# evaluate_cot(model1,tokenizer1,prompts_cot,\"Gemma\",dataset)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T08:19:55.635646Z","iopub.execute_input":"2024-09-22T08:19:55.636016Z","iopub.status.idle":"2024-09-22T08:19:55.646136Z","shell.execute_reply.started":"2024-09-22T08:19:55.635977Z","shell.execute_reply":"2024-09-22T08:19:55.645215Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"evaluate_cot(model2,tokenizer2,prompts_cot,\"Phi-3.5\",dataset)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T08:19:55.647370Z","iopub.execute_input":"2024-09-22T08:19:55.647784Z","iopub.status.idle":"2024-09-22T08:36:03.720450Z","shell.execute_reply.started":"2024-09-22T08:19:55.647747Z","shell.execute_reply":"2024-09-22T08:36:03.718865Z"},"trusted":true},"execution_count":21,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluate_cot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtokenizer2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprompts_cot\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPhi-3.5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[16], line 5\u001b[0m, in \u001b[0;36mevaluate_cot\u001b[0;34m(model, tokenizer, prompts, model_name, dataset)\u001b[0m\n\u001b[1;32m      3\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m----> 5\u001b[0m     responses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      7\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      8\u001b[0m inference_time \u001b[38;5;241m=\u001b[39m (end \u001b[38;5;241m-\u001b[39m start)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m100\u001b[39m\n","Cell \u001b[0;32mIn[9], line 5\u001b[0m, in \u001b[0;36mgenerate_response\u001b[0;34m(prompt, model, tokenizer, new_tokens)\u001b[0m\n\u001b[1;32m      2\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 5\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m response \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2024\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2016\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2017\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2018\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2019\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2020\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2021\u001b[0m     )\n\u001b[1;32m   2023\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2024\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2032\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2036\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   2037\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2038\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2039\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[1;32m   2040\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2041\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2982\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2979\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   2981\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2982\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2985\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/phi3/modeling_phi3.py:1260\u001b[0m, in \u001b[0;36mPhi3ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1247\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m   1248\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1249\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1256\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1257\u001b[0m )\n\u001b[1;32m   1259\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 1260\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1261\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m   1263\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 24.12 MiB is free. Process 2231 has 14.71 GiB memory in use. Of the allocated memory 14.52 GiB is allocated by PyTorch, and 69.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 24.12 MiB is free. Process 2231 has 14.71 GiB memory in use. Of the allocated memory 14.52 GiB is allocated by PyTorch, and 69.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}]},{"cell_type":"code","source":"# evaluate_cot(model3,tokenizer3,prompts_cot,\"Llama-3.1\",dataset)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T08:36:03.721590Z","iopub.status.idle":"2024-09-22T08:36:03.722125Z","shell.execute_reply.started":"2024-09-22T08:36:03.721834Z","shell.execute_reply":"2024-09-22T08:36:03.721876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# responses = []\n# start = time.time()\n# j=0\n# for i in prompts_cot:\n#     responses.append(generate_response(i,model1,tokenizer1,150))\n#     print(responses[j])\n#     j=j+1\n\n# end = time.time()\n# inference_time = (end - start)/100\n# print(f\"Inference time for cot for gemma: {inference_time:.6f} seconds\")\n    ","metadata":{"execution":{"iopub.status.busy":"2024-09-22T08:36:03.723141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# correct_count = 0\n# for i, example in enumerate(dataset):\n#     extracted_answer = extract_answer(responses[i])\n#     if extracted_answer==None:\n#         extracted_answer = \"??\"\n\n#     if extracted_answer in options_val:\n#             extracted_answer = example['choices'][int(extracted_answer[7])-1]\n\n#     actual_answer = example['choices'][example['answer']]\n#     print(extracted_answer)\n#     print(actual_answer)\n    \n#     if extracted_answer in actual_answer or actual_answer in extracted_answer:\n#         correct_count += 1\n#         print(\"HA\")\n\n# accuracy = correct_count / len(dataset)\n# print(f\"Accuracy of {model_name} for zero shot: {accuracy:.2f}\")    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}